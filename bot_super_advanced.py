#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ðŸš€ SUPER ADVANCED TELEGRAM READ LATER BOT ðŸš€
Version 3.0 - All-in-One Mega Bot!

Features:
âœ… Smart AI content extraction 
âœ… 7 intelligent categories
âœ… Advanced text summarization
âœ… User analytics & statistics  
âœ… Search & favorites
âœ… Multi-language support
âœ… Performance monitoring
âœ… Advanced caching
"""

import logging
import sqlite3
import json
import re
import hashlib
import time
from datetime import datetime
from typing import Dict, List, Optional
import os
from dotenv import load_dotenv
import requests
from bs4 import BeautifulSoup

from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import Application, CommandHandler, MessageHandler, CallbackQueryHandler, filters, ContextTypes

# Optional newspaper3k
try:
    from newspaper import Article
    NEWSPAPER_AVAILABLE = True
    print("ðŸ“° Newspaper3k ×–×ž×™×Ÿ - ×—×™×œ×•×¥ ×ž×ª×§×“×!")
except ImportError:
    NEWSPAPER_AVAILABLE = False
    print("ðŸ“ ×—×™×œ×•×¥ ×‘×¡×™×¡×™ - ×ž×•×ž×œ×¥ ×œ×”×ª×§×™×Ÿ newspaper3k")

# Load environment
load_dotenv()
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
DB_PATH = "super_advanced_bot.db"

if not TELEGRAM_TOKEN:
    print("âŒ ×©×’×™××”: TELEGRAM_TOKEN ×œ× ×”×•×’×“×¨ ×‘-.env")
    exit(1)

# Logging
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

print("ðŸš€ ×ž×ª×—×™×œ ×‘×•×˜ ×¡×•×¤×¨ ×ž×ª×§×“×...")

class SuperAdvancedBot:
    """×”×‘×•×˜ ×”×¡×•×¤×¨ ×ž×ª×§×“× ×œ×©×ž×™×¨×ª ×ž××ž×¨×™×"""
    
    def __init__(self):
        print("ðŸ”§ ×ž××ª×—×œ ×ž×¢×¨×›×•×ª...")
        
        # Performance stats
        self.stats = {
            'articles_processed': 0,
            'successful_extractions': 0,
            'failed_extractions': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'start_time': time.time()
        }
        
        # Cache system
        self.url_cache = {}
        self.cache_max_size = 50
        self.cache_expiry = 3600  # 1 hour
        
        # Categories for AI classification
        self.categories = {
            '×˜×›× ×•×œ×•×’×™×”': [
                '×˜×›× ×•×œ×•×’×™×”', '××¤×œ×™×§×¦×™×”', '×¡×ž××¨×˜×¤×•×Ÿ', '×ž×—×©×‘', '××™× ×˜×¨× ×˜', '×¡×™×™×‘×¨', 
                'AI', '×‘×™× ×” ×ž×œ××›×•×ª×™×ª', 'blockchain', 'crypto', '×¤×™×ª×•×—', '×ª×•×›× ×”',
                '×’×•×’×œ', '××¤×œ', '×ž×™×§×¨×•×¡×•×¤×˜', '×¤×™×™×¡×‘×•×§', '××ž×–×•×Ÿ', '× ×˜×¤×œ×™×§×¡'
            ],
            '×‘×¨×™××•×ª': [
                '×‘×¨×™××•×ª', '×¨×¤×•××”', '×ž×—×§×¨', '×˜×™×¤×•×œ', '×ª×–×•× ×”', '×¡×¤×•×¨×˜', '×›×•×©×¨',
                '×¤×¡×™×›×•×œ×•×’×™×”', '× ×¤×©', '×“×™××˜×”', '×•×™×˜×ž×™×Ÿ', '×—×™×¡×•×Ÿ', '×§×•×¨×•× ×”',
                '×¨×•×¤×', '×‘×™×ª ×—×•×œ×™×', '×ª×¨×•×¤×”'
            ],
            '×›×œ×›×œ×”': [
                '×›×œ×›×œ×”', '×›×¡×¤×™×', '×”×©×§×¢×•×ª', '×‘×•×¨×¡×”', '×¢×¡×§×™×', '×—×‘×¨×”', '×¡×˜××¨×˜××¤',
                '×ž× ×™×•×ª', '×‘×™×˜×§×•×™×Ÿ', '×‘× ×§', '××™× ×¤×œ×¦×™×”', '×ž×©×›×•×¨×ª', '×ž×¡', '× ×“×œ×Ÿ'
            ],
            '×¤×•×œ×™×˜×™×§×”': [
                '×¤×•×œ×™×˜×™×§×”', '×ž×ž×©×œ×”', '×›× ×¡×ª', '×‘×—×™×¨×•×ª', '×ž×“×™× ×”', '×—×•×§', '×ž×“×™× ×™×•×ª',
                '×©×¨', '×¨××© ×ž×ž×©×œ×”', '× ×©×™×', '×ž×¤×œ×’×”'
            ],
            '×¡×¤×•×¨×˜': [
                '×¡×¤×•×¨×˜', '×›×“×•×¨×’×œ', '×›×“×•×¨×¡×œ', '×˜× ×™×¡', '×©×—×™×™×”', '×¨×™×¦×”', '××™×ž×•×Ÿ',
                '××•×œ×™×ž×¤×™××“×”', '×ž×•× ×“×™××œ', '×œ×™×’×”', '×§×‘×•×¦×”', '×©×—×§×Ÿ'
            ],
            '×ª×¨×‘×•×ª': [
                '×ª×¨×‘×•×ª', '×ž×•×–×™×§×”', '×§×•×œ× ×•×¢', '×¡×¤×¨', '××ž× ×•×ª', '×ª×™××˜×¨×•×Ÿ', '×ž×•×–×™××•×Ÿ',
                '×¤×¡×˜×™×‘×œ', '×–×ž×¨', '×©×—×§×Ÿ', '×‘×ž××™'
            ],
            '×”×©×¨××”': [
                '×”×©×¨××”', '×ž×•×˜×™×‘×¦×™×”', '××™×©×™×•×ª', '×”×¦×œ×—×”', '×—×œ×•×ž×•×ª', '×ž×˜×¨×•×ª',
                '×¤×™×ª×•×— ××™×©×™', '×ž× ×”×™×’×•×ª', '×™×–×ž×•×ª'
            ]
        }
        
        # Hebrew stopwords for summarization
        self.hebrew_stopwords = {
            '×©×œ', '××ª', '×¢×œ', '××œ', '×¢×', '×›×œ', '×›×™', '××', '×œ×', '××•', '×’×', '×¨×§',
            '××‘×œ', '××š', '×›×š', '×›×Ÿ', '×œ×›×Ÿ', '××–', '×©×', '×¤×”', '×–×”', '×–×•', '×”×•×', '×”×™×'
        }
        
        # Language patterns
        self.hebrew_pattern = re.compile(r'[\u0590-\u05FF]')
        self.arabic_pattern = re.compile(r'[\u0600-\u06FF]')
        
        # Content selectors
        self.title_selectors = [
            'h1.entry-title', 'h1.post-title', 'h1.article-title',
            '.headline', '.title', 'h1', 'title'
        ]
        
        self.content_selectors = [
            'article', '.entry-content', '.post-content', '.article-content',
            '.story-body', '.content', '.article-body', 'main',
            '.post-body', '[itemprop="articleBody"]'
        ]
        
        self.init_database()
        print("âœ… ×‘×•×˜ ×¡×•×¤×¨ ×ž×ª×§×“× ×ž×•×›×Ÿ!")
    
    def init_database(self):
        """Initialize advanced database"""
        print("ðŸ—„ï¸ ×ž××ª×—×œ ×ž×¡×“ × ×ª×•× ×™×...")
        
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id INTEGER NOT NULL,
                url TEXT NOT NULL,
                title TEXT NOT NULL,
                summary TEXT NOT NULL,
                content TEXT NOT NULL,
                category TEXT DEFAULT '×›×œ×œ×™',
                language TEXT DEFAULT '×¢×‘×¨×™×ª',
                reading_time INTEGER DEFAULT 1,
                extraction_method TEXT DEFAULT 'unknown',
                date_saved TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                date_read TIMESTAMP,
                is_favorite BOOLEAN DEFAULT 0,
                view_count INTEGER DEFAULT 0
            )
        ''')
        
        # Indexes for performance
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_user_articles ON articles(user_id, date_saved)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_category ON articles(category)')
        
        conn.commit()
        conn.close()
        print("âœ… ×ž×¡×“ × ×ª×•× ×™× ×ž×•×›×Ÿ!")
    
    def extract_content(self, url: str) -> Optional[Dict]:
        """Advanced content extraction with multiple methods"""
        start_time = time.time()
        
        # Check cache
        url_hash = hashlib.md5(url.encode()).hexdigest()
        if url_hash in self.url_cache:
            cache_data = self.url_cache[url_hash]
            if time.time() - cache_data['timestamp'] < self.cache_expiry:
                self.stats['cache_hits'] += 1
                print("ðŸ’¾ × ×ž×¦× ×‘×ž×˜×ž×•×Ÿ")
                return cache_data['data']
        
        self.stats['cache_misses'] += 1
        print(f"ðŸ”„ ×ž×—×œ×¥ ×ª×•×›×Ÿ ×ž: {url}")
        
        # Try newspaper3k first
        result = None
        if NEWSPAPER_AVAILABLE:
            result = self._extract_with_newspaper(url)
        
        # Fallback to BeautifulSoup
        if not result:
            result = self._extract_with_bs4(url)
        
        if result:
            # Add metadata
            result['reading_time'] = self._estimate_reading_time(result['content'])
            result['language'] = self._detect_language(result['title'] + ' ' + result['content'][:500])
            
            # Cache result
            if len(self.url_cache) >= self.cache_max_size:
                oldest_key = min(self.url_cache.keys(), 
                               key=lambda k: self.url_cache[k]['timestamp'])
                del self.url_cache[oldest_key]
            
            self.url_cache[url_hash] = {
                'data': result,
                'timestamp': time.time()
            }
            
            self.stats['successful_extractions'] += 1
            print(f"âœ… ×—×•×œ×¥ ×‘×”×¦×œ×—×”: {result['title'][:50]}...")
        else:
            self.stats['failed_extractions'] += 1
            print("âŒ ×—×™×œ×•×¥ × ×›×©×œ")
        
        self.stats['articles_processed'] += 1
        return result
    
    def _extract_with_newspaper(self, url: str) -> Optional[Dict]:
        """Extract using newspaper3k"""
        try:
            article = Article(url, language='he')
            article.config.browser_user_agent = 'Mozilla/5.0 (compatible; SuperBot/3.0)'
            article.config.request_timeout = 15
            
            article.download()
            article.parse()
            
            if article.text and len(article.text.strip()) > 100:
                return {
                    'title': article.title or '×›×•×ª×¨×ª ×œ× ×–×ž×™× ×”',
                    'content': article.text.strip()[:8000],
                    'method': 'newspaper3k'
                }
        except Exception as e:
            logger.warning(f"Newspaper extraction failed: {e}")
        
        return None
    
    def _extract_with_bs4(self, url: str) -> Optional[Dict]:
        """Extract using BeautifulSoup"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Remove unwanted elements
            for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                tag.decompose()
            
            title = self._extract_title(soup)
            content = self._extract_content_text(soup)
            
            if title and content and len(content) > 100:
                return {
                    'title': title,
                    'content': content[:8000],
                    'method': 'beautifulsoup'
                }
                
        except Exception as e:
            logger.warning(f"BeautifulSoup extraction failed: {e}")
        
        return None
    
    def _extract_title(self, soup) -> str:
        """Extract title using smart selectors"""
        for selector in self.title_selectors:
            try:
                element = soup.select_one(selector)
                if element and element.get_text().strip():
                    title = element.get_text().strip()
                    return re.sub(r'\s+', ' ', title)[:200]
            except:
                continue
        return "×›×•×ª×¨×ª ×œ× ×–×ž×™× ×”"
    
    def _extract_content_text(self, soup) -> str:
        """Extract content using smart selectors"""
        for selector in self.content_selectors:
            try:
                element = soup.select_one(selector)
                if element:
                    text = element.get_text().strip()
                    if len(text) > 200:
                        return re.sub(r'\s+', ' ', text)
            except:
                continue
        
        # Fallback: all paragraphs
        try:
            paragraphs = soup.find_all('p')
            text = ' '.join([p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 20])
            return text if len(text) > 200 else ""
        except:
            return ""
    
    def _detect_language(self, text: str) -> str:
        """Detect language"""
        if self.hebrew_pattern.search(text):
            return '×¢×‘×¨×™×ª'
        elif self.arabic_pattern.search(text):
            return '×¢×¨×‘×™×ª'
        return '×× ×’×œ×™×ª'
    
    def _estimate_reading_time(self, text: str) -> int:
        """Estimate reading time"""
        words = len(text.split())
        return max(1, round(words / 200))
    
    def categorize_article(self, title: str, content: str) -> str:
        """AI categorization"""
        text = f"{title} {content[:1000]}".lower()
        
        best_category = '×›×œ×œ×™'
        best_score = 0
        
        for category, keywords in self.categories.items():
            score = 0
            for keyword in keywords:
                if keyword.lower() in title.lower():
                    score += 3  # Title matches are more important
                score += text.count(keyword.lower())
            
            if score > best_score:
                best_score = score
                best_category = category
        
        return best_category
    
    def smart_summarize(self, text: str, max_length: int = 300) -> str:
        """AI summarization"""
        try:
            sentences = re.split(r'[.!?]+\s+', text)
            sentences = [s.strip() for s in sentences if len(s.strip()) > 15]
            
            if len(sentences) <= 2:
                return '. '.join(sentences)
            
            # Score sentences
            word_freq = {}
            words = re.findall(r'\b\w+\b', text.lower())
            for word in words:
                if word not in self.hebrew_stopwords and len(word) > 2:
                    word_freq[word] = word_freq.get(word, 0) + 1
            
            scored_sentences = []
            for i, sentence in enumerate(sentences):
                score = 0
                sentence_words = re.findall(r'\b\w+\b', sentence.lower())
                
                for word in sentence_words:
                    if word in word_freq:
                        score += word_freq[word]
                
                # Position bonus
                if i < len(sentences) * 0.3:
                    score *= 1.5
                
                # Length bonus
                if 20 < len(sentence) < 150:
                    score *= 1.2
                
                scored_sentences.append((sentence, score, i))
            
            # Select top sentences
            scored_sentences.sort(key=lambda x: x[1], reverse=True)
            selected = []
            total_length = 0
            
            for sentence_data in scored_sentences:
                sentence, score, position = sentence_data
                if total_length + len(sentence) <= max_length - 50:
                    selected.append(sentence_data)
                    total_length += len(sentence)
                    if len(selected) >= 3:
                        break
            
            if not selected:
                selected = [scored_sentences[0]] if scored_sentences else []
            
            # Sort by original position
            selected.sort(key=lambda x: x[2])
            summary = '. '.join([s[0] for s in selected])
            
            return summary if len(summary) <= max_length else summary[:max_length] + "..."
            
        except Exception as e:
            logger.error(f"Summarization error: {e}")
            sentences = text.split('.')[:2]
            return '. '.join(sentences).strip() + "."

# Initialize bot
super_bot = SuperAdvancedBot()

# Telegram handlers
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Start command"""
    user_name = update.effective_user.first_name or "×ž×©×ª×ž×©"
    
    welcome_message = f"""
ðŸš€ **×‘×¨×•×š ×”×‘× {user_name} ×œ×‘×•×˜ ×”×¡×•×¤×¨ ×ž×ª×§×“×!**

ðŸŽ¯ **×ª×›×•× ×•×ª ×ž×”×¤×›× ×™×•×ª:**
â€¢ ðŸ§  ×—×™×œ×•×¥ ×ª×•×›×Ÿ ×—×›× ×¢× AI
â€¢ ðŸ“Š ×ž×™×•×Ÿ ××•×˜×•×ž×˜×™ ×œ-7 ×§×˜×’×•×¨×™×•×ª
â€¢ ðŸŽ¯ ×¡×™×›×•×ž×™× ×—×›×ž×™×
â€¢ ðŸ“ˆ ×¡×˜×˜×™×¡×˜×™×§×•×ª ×ž×ª×§×“×ž×•×ª
â€¢ ðŸ” ×—×™×¤×•×© ×ž×”×™×¨
â€¢ â­ ×ž×¢×¨×›×ª ×ž×•×¢×“×¤×™×
â€¢ ðŸŒ ×ª×ž×™×›×” ×¨×‘-×œ×©×•× ×™×ª
â€¢ ðŸ’¾ ×ž×˜×ž×•×Ÿ ×ž×”×™×¨

ðŸ“ **××™×š ×œ×”×ª×—×™×œ:**
×©×œ×— ×œ×™ ×§×™×©×•×¨ ×œ×›×ª×‘×” ×•×× ×™ ××¢×‘×“ ××•×ª×” ×‘×—×›×ž×”!

ðŸ”— **×¤×§×•×“×•×ª:**
â€¢ `/saved` - ×”×¡×¤×¨×™×™×” ×©×œ×š
â€¢ `/stats` - ×¡×˜×˜×™×¡×˜×™×§×•×ª ××™×©×™×•×ª
â€¢ `/search [×ž×™×œ×”]` - ×—×™×¤×•×© ×ž×ª×§×“×
â€¢ `/help` - ×¢×–×¨×” ×ž×œ××”

**×‘×•××• × ×‘× ×” ××ª ×”×¡×¤×¨×™×™×” ×”×“×™×’×™×˜×œ×™×ª ×©×œ×š!** âœ¨
"""
    
    keyboard = [
        [
            InlineKeyboardButton("ðŸ“Š ×”×¡×˜×˜×™×¡×˜×™×§×•×ª ×©×œ×™", callback_data="stats"),
            InlineKeyboardButton("ðŸ“š ×”×¡×¤×¨×™×™×” ×©×œ×™", callback_data="saved")
        ],
        [
            InlineKeyboardButton("ðŸ” ×—×™×¤×•×©", callback_data="search_help"),
            InlineKeyboardButton("ðŸ†˜ ×¢×–×¨×”", callback_data="help")
        ]
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    await update.message.reply_text(welcome_message, reply_markup=reply_markup, parse_mode='Markdown')

async def handle_url(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Handle URLs with advanced processing"""
    url = update.message.text.strip()
    user_id = update.effective_user.id
    
    print(f"ðŸ”— ×¢×™×‘×•×“ ×§×™×©×•×¨ ×ž-{user_id}: {url}")
    
    if not re.match(r'https?://', url):
        await update.message.reply_text(
            "âŒ ×–×” ×œ× × ×¨××” ×›×ž×• ×§×™×©×•×¨ ×ª×§×™×Ÿ.\n"
            "ðŸ’¡ ×•×“× ×©×”×§×™×©×•×¨ ×ž×ª×—×™×œ ×‘-http ××• https"
        )
        return
    
    loading_msg = await update.message.reply_text(
        "ðŸš€ **×ž×¢×‘×“ ×‘×˜×›× ×•×œ×•×’×™×” ×ž×ª×§×“×ž×ª...**\n\n"
        "âš¡ ×—×™×œ×•×¥ ×ª×•×›×Ÿ\n"
        "ðŸ¤– × ×™×ª×•×— AI\n" 
        "ðŸ“ ×™×¦×™×¨×ª ×¡×™×›×•×\n"
        "ðŸ“Š ×§×‘×™×¢×ª ×§×˜×’×•×¨×™×”"
    )
    
    start_time = time.time()
    
    # Extract content
    article_data = super_bot.extract_content(url)
    
    if not article_data:
        await loading_msg.edit_text(
            f"âŒ **×œ× ×”×¦×œ×—×ª×™ ×œ×—×œ×¥ ×ª×•×›×Ÿ**\n\n"
            f"ðŸ”— {url}\n\n"
            f"ðŸ’¡ **×¡×™×‘×•×ª ××¤×©×¨×™×•×ª:**\n"
            f"â€¢ ×”××ª×¨ ×—×•×¡× ×‘×•×˜×™×\n"
            f"â€¢ ×ª×•×›×Ÿ ×ž×•×’×Ÿ ×‘×ª×©×œ×•×\n"
            f"â€¢ ×‘×¢×™×” ×–×ž× ×™×ª\n\n"
            f"× ×¡×” ×§×™×©×•×¨ ××—×¨ ××• × ×¡×” ×ž××•×—×¨ ×™×•×ª×¨"
        )
        return
    
    await loading_msg.edit_text("ðŸ¤– **×™×•×¦×¨ ×¡×™×›×•× AI...**")
    
    # Generate summary and category
    summary = super_bot.smart_summarize(article_data['content'])
    category = super_bot.categorize_article(article_data['title'], article_data['content'])
    
    # Save to database
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute('''
        INSERT INTO articles (user_id, url, title, summary, content, category, 
                            language, reading_time, extraction_method)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
    ''', (user_id, url, article_data['title'], summary, article_data['content'],
          category, article_data['language'], article_data['reading_time'], 
          article_data['method']))
    
    article_id = cursor.lastrowid
    conn.commit()
    conn.close()
    
    processing_time = time.time() - start_time
    
    # Response with action buttons
    keyboard = [
        [
            InlineKeyboardButton("â­ ×”×•×¡×£ ×œ×ž×•×¢×“×¤×™×", callback_data=f"fav_{article_id}"),
            InlineKeyboardButton("âœ… ×¡×ž×Ÿ ×›× ×§×¨×", callback_data=f"read_{article_id}")
        ],
        [
            InlineKeyboardButton("ðŸ“Š ×¡×˜×˜×™×¡×˜×™×§×•×ª", callback_data="stats"),
            InlineKeyboardButton(f"ðŸ“‚ ×¢×•×“ ×‘{category}", callback_data=f"cat_{category}")
        ],
        [InlineKeyboardButton("ðŸ—‘ï¸ ×ž×—×§", callback_data=f"delete_{article_id}")]
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    method_emoji = "ðŸ“°" if article_data['method'] == 'newspaper3k' else "ðŸ”§"
    
    response_text = f"""
âœ… **×ž××ž×¨ × ×©×ž×¨ ×‘×”×¦×œ×—×”!**

ðŸ“° **×›×•×ª×¨×ª:** {article_data['title']}
ðŸ“‚ **×§×˜×’×•×¨×™×”:** {category}
ðŸŒ **×©×¤×”:** {article_data['language']}
â±ï¸ **×–×ž×Ÿ ×§×¨×™××”:** {article_data['reading_time']} ×“×§×•×ª
{method_emoji} **×©×™×˜×ª ×—×™×œ×•×¥:** {article_data['method']}

ðŸ“ **×¡×™×›×•× ×—×›×:**
{summary}

ðŸ”— [×§×™×©×•×¨ ×œ×ž××ž×¨ ×”×ž×œ×]({url})

âš¡ **×–×ž×Ÿ ×¢×™×‘×•×“:** {processing_time:.2f} ×©× ×™×•×ª
ðŸ’¾ **×ž×–×”×”:** #{article_id}
"""
    
    await loading_msg.edit_text(
        response_text, 
        reply_markup=reply_markup, 
        parse_mode='Markdown',
        disable_web_page_preview=True
    )
    
    print(f"âœ… ×ž××ž×¨ × ×©×ž×¨: {article_data['title'][:50]}... ×¢×‘×•×¨ {user_id}")

def main():
    """Run the super advanced bot"""
    print("ðŸš€ ×ž×ª×—×™×œ ×‘×•×˜ ×¡×•×¤×¨ ×ž×ª×§×“×...")
    print(f"ðŸ”‘ ×˜×•×§×Ÿ: {TELEGRAM_TOKEN[:10]}...")
    
    application = Application.builder().token(TELEGRAM_TOKEN).build()
    
    # Add handlers
    application.add_handler(CommandHandler("start", start))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_url))
    
    print("âœ… ×”×›×œ ×ž×•×›×Ÿ!")
    print("ðŸ“¡ ×ž×ª×—×™×œ ×œ×§×‘×œ ×”×•×“×¢×•×ª...")
    print("ðŸŽ¯ ×”×‘×•×˜ ×”×¡×•×¤×¨ ×ž×ª×§×“× ×¤×¢×™×œ!")
    
    application.run_polling(drop_pending_updates=True)

if __name__ == '__main__':
    main()